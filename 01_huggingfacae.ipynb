{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_Hugging Face\n",
    "---\n",
    "## 01_01 Hugging Face란?\n",
    "* 코드로 구현되어 있는 모델을 모아놓은 곳으로 AI의 github라 불린다.\n",
    "* 허깅페이스는 최신 AI 기술을 더 간편하게 활용 할 수 있또록 제공되는 플랫폼이며, 핵심기술은 Transformers 라이브러리이다.\n",
    "\n",
    "### 01_01_01 Hugging Face 장점\n",
    "* 모델을 처음부터 개발하고 훈련할 필요가 없다.\n",
    "* 사용 편의성 (간단하고 사용자 친화적인 인터페이스가 제공되어 쉽게 이용가능)\n",
    "* 지속적으로 발전 중 (AI의 최신기술을 공유받고 쉽게 접근 가능)\n",
    "\n",
    "\n",
    "## 01_01 Transformers 라이브러리\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01_01_01 자연어처리\n",
    "* 컴퓨터가 인간의 언어를 이해, 생성, 조작 할 수 있도록 해주는 인공지능의 한 분야\n",
    "\n",
    "**자연어 처리의 주요 단계**\n",
    "\n",
    "* 토큰화: 텍스트를 의미 있는 단위(토큰)로 나누는 과정  \n",
    "    예시: \"나는 학교에 간다\" -> [\"나는\", \"학교에\", \"간다\"]<br>\n",
    "\n",
    "* 단어 집합 생성: 텍스트에서 고유한 단어들을 모아 사전을 만드는 과정  \n",
    "    예시: [\"나는\", \"학교에\", \"간다\"] -> {\"나는\", \"학교에\", \"간다\"}<br>\n",
    "\n",
    "* 정수 인코딩: 단어를 고유한 정수로 변환하여 컴퓨터가 처리할 수 있게 하는 과정  \n",
    "    예시: {\"나는\": 1, \"학교에\": 2, \"간다\": 3} -> [1, 2, 3]<br>\n",
    "\n",
    "* 패딩: 서로 다른 길이의 시퀀스를 동일한 길이로 맞추는 과정  \n",
    "    예시: [1, 2, 3] -> [1, 2, 3, 0, 0] (길이 5로 패딩)<br>\n",
    "\n",
    "* 벡터화: 텍스트 데이터를 숫자 벡터로 변환하여 기계학습 모델에 입력할 수 있게 하는 과정  \n",
    "    예시: \"나는\" -> [0.1, 0.2, 0.3, ..., 0.9] (100차원 벡터)  \n",
    "\n",
    "<br><br>\n",
    "\n",
    "**형태소 토큰화**  \n",
    "<br>\n",
    "원시문 예시 :  \n",
    "\"길을 가다가 고양이를 봤어. 고양이에게 츄르를 주려했는데 고양이가 도망가버렸어\"  \n",
    "\n",
    "토큰화 예시 :  \n",
    "[\"길을\", \"가다가\", \"고양이를\", \"봤어\", \".\", \"고양이에게\", \"츄르를\", \"주려했는데\", \"고양이가\", \"도망가버렸어\"]  \n",
    "\n",
    "똑같은 고양이가 총 3번 등장한다, 이때 \"를\", \"에게\", \"가\" 가 붙어있어 기계는 각기 다른 단어로 인식 할 수 있다.  \n",
    "따라서 한국에서는 보편적으로 '형태소 분석기'로 토큰화를 진행한다.  \n",
    "\n",
    "형태소 토큰화 예시 : \n",
    "[\"길\", \"을\", \"가다가\", \"고양이\", \"를\", \"보\", \"았\", \"어\", \".\", \"고양이\", \"에게\", \"츄르\", \"를\", \"주\", \"려\", \"하\", \"았\", \"는데\", \"고양이\", \"가\", \"도망가\", \"버리\", \"었\", \"어\"]  \n",
    "\n",
    "이렇게 형태소 단위로 분리하면 \"고양이\"라는 단어가 일관되게 인식되며, 조사나 어미는 별도로 분리된다.  \n",
    "이를 통해 기계가 더 정확하게 단어의 의미와 문법적 기능을 파악할 수 있다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자연어 처리 발전 과정\n",
    "\n",
    "**카운트 기반 단어 표현**\n",
    "* 단어 표현 중 하나로, 단어의 빈도수를 기반으로 단어의 의미를 표현하는 방법\n",
    "* DTM(Document Term Matrix) : 문서 내 단어의 빈도수를 기반으로 문서를 표현하는 방법\n",
    "* TF-IDF : 단어의 중요도를 고려하여 문서를 표현하는 방법 <br>\n",
    "\n",
    "Bag of Words(단어주머니)\n",
    "* 문서 내 단어의 출현 여부를 기반으로 문서를 표현하는 방법\n",
    "* 단어의 출현 빈도로만 나타내기 때문에 단어의 순서는 고려하지 않는다.\n",
    "<br> => 문맥 파악이 불가능 (강아지가 놀고 있다 와 놀고 있는 강아지를 구별 못함)\n",
    "\n",
    "**단어임베딩(통계기반의 언어모델)**\n",
    "* 대표적인 알고리즘 : Word2vec, GloVe\n",
    "* 단어를 벡터로 표현하는 방법\n",
    "* 단어의 의미를 벡터로 표현하여 문맥을 고려할 수 있도록 해준다.\n",
    "<br> => 하지만 다의어 같은 (문맥에따라 다양한의미를 기지는걸 구별하지 못함)\n",
    "\n",
    "**딥러닝 기반 언어 모델**\n",
    "\n",
    "**시퀀스 모델**\n",
    "* 시퀀스란?\n",
    "<br>시퀀스는 순서가 있는 데이터의 집합을 의미한다.\n",
    "<br>자연어 처리에서 시퀀스는 주로 단어나 문자의 연속을 나타낸다.\n",
    "<br>예를 들어, 문장은 단어들의 시퀀스이고, 단어는 문자들의 시퀀스입니다.\n",
    "<br>시퀀스 데이터는 순서가 중요하며, 이전 요소가 다음 요소에 영향을 미칠 수 있다.<br>\n",
    "\n",
    "* RNN(Recurrent Neural Network)<br> \n",
    "순차적 데이터를 처리하는 신경망으로, 이전 단계의 정보를 현재 단계로 전달하는 구조를 가짐\n",
    "* LSTM(Long Short-Term Memory)<br>\n",
    "RNN의 장기 의존성 문제를 해결하기 위해 개발된 모델로, 장기 기억을 유지할 수 있는 구조를 가짐\n",
    "* GRU(Gated Recurrent Unit)<br>\n",
    "LSTM을 간소화한 모델로, 더 적은 매개변수를 사용하면서도 비슷한 성능을 보임\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01_01_02 Transformers 라이브러리란?\n",
    "* 허깅페이스에서 제공하는 최신 모델을 쉽게 사용할 수 있도록 도와주는 라이브러리이다.\n",
    "* 트랜스포머는 자연어 처리 분야에서 많이 사용되는 모델로, 다양한 언어 모델을 포함하고 있다.\n",
    "\n",
    "트랜스포머는 자연어 처리 분야의 주요 딥러닝 모델로, \"Attention is All You Need\" 논문에서 소개되었다.  \n",
    "Self-Attention 메커니즘을 사용하여 문맥 이해와 단어 간 관계 파악에 효과적이다.  \n",
    "기계 번역, 텍스트 생성, 질문 응답 등 다양한 작업에서 우수한 성능을 보인다.\n",
    "\n",
    "쉽게 생각하면 컴퓨터가 말하는 방법을 배우는 것과 비슷한 것으로 문장 속의 단어들이 어떤 관계를 가지고 있는지 파악하고 문장의 뜻을 이해하는데 기술이다\n",
    "\n",
    "* Transformers를 통해 사전학습된 최신 모델을 쉽게 다운로드하고 학습할 수 있는 API와 도구를 제공한다.\n",
    "\n",
    "**지원하는 기능**\n",
    "1) 자연어 처리 : 텍스트 분류, 엔티티 인식, 질문답변, 언어모델링, 요약, 번역, 객관식 텍스트 생성\n",
    "2) 컴퓨터 비전 : 이미지 분류, 물체감지 및 분할\n",
    "3) 오디오 : 음성인식 및 오디오 분류\n",
    "4) 멀티모달 : 테이블 질문 답변, 광학문자 인식, 스캔문서에서 정보추출, 비디오분류, 시각적 질문답변\n",
    "\n",
    "멀티모달이란 : 텍스트 이미지, 음성, 영상 등 다양한 양식으로 훈련해 다양한 결과물을 낼 수 있는 모델\n",
    "-> 즉, 여러가지 유형의 데이터를 동시에 다루는 기술\n",
    "\n",
    "https://huggingface.co/docs/transformers/index\n",
    "\n",
    "파이토치\n",
    "https://pytorch.kr/?_gl=1*10j0jf8*_ga*OTk5MTU5Njg2LjE3Mjk1NTk1NzY.*_ga_LZRD6GXDLF*MTcyOTU1OTU3NS4xLjAuMTcyOTU1OTU3NS42MC4wLjA.\n",
    "\n",
    "트랜스포머스\n",
    "pip install transfomers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1115, 0.5014, 0.1522],\n",
      "        [0.9156, 0.2485, 0.3387],\n",
      "        [0.4948, 0.6335, 0.1742],\n",
      "        [0.9821, 0.5904, 0.2759],\n",
      "        [0.9054, 0.0963, 0.9442]])\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(transformers\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(5,3)\n",
    "print(x)\n",
    "\n",
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
