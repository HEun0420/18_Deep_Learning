{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지역 이미지를 넣어서 해당 이미지가 어느 지역인지를 추론하는 모델을 활용한 fast API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지역 이름을 입력하세요. 입력을 끝내려면 'x'를 입력하세요.\n",
      "입력하신 지역들은 이미지와 유사도가 낮아 정확하게 찾아줄 수는 없습니다.\n",
      " 그러나 이 이미지는 현재 입력하신 지역 중 파리가 가장 유사도가 높습니다. 유사도는 51.05%입니다.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from googletrans import Translator\n",
    "\n",
    "# 모델과 프로세서 로드\n",
    "model = CLIPModel.from_pretrained(\"geolocal/StreetCLIP\")\n",
    "processor = CLIPProcessor.from_pretrained(\"geolocal/StreetCLIP\")\n",
    "\n",
    "# 이미지 불러오기\n",
    "url = \"https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# 지역 이름 입력받기\n",
    "choices = []\n",
    "original_choices = []  # 원래 입력한 지역 이름을 저장할 리스트\n",
    "translator = Translator()  # 번역기 초기화\n",
    "\n",
    "while True:\n",
    "    print(\"지역 이름을 입력하세요. 입력을 끝내려면 'x'를 입력하세요.\")\n",
    "    \n",
    "    while True:\n",
    "        location = input(\"지역 이름: \")\n",
    "        if location.lower() == \"x\":\n",
    "            break\n",
    "        if location:  # 빈 입력이 아닌 경우만 추가\n",
    "            translated_location = translator.translate(location, src='ko', dest='en').text\n",
    "            choices.append(translated_location)  # 번역된 지역 이름 추가\n",
    "            original_choices.append(location)  # 원래 지역 이름 추가\n",
    "\n",
    "    # 지역 이름이 입력되지 않았을 경우 재입력 요청\n",
    "    if not choices:\n",
    "        print(\"입력된 지역이 없습니다. 정확한 추론을 위해 한 가지 이상의 지역을 입력해주세요.\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 이미지와 입력한 텍스트를 모델에 전달\n",
    "inputs = processor(text=choices, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# 모델 추론\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "# logits_per_image에서 가장 큰 값의 인덱스 구하기\n",
    "max_index = torch.argmax(logits_per_image).item()\n",
    "\n",
    "# 가장 높은 유사도를 가진 지역과 유사도 값\n",
    "most_similar_location = original_choices[max_index]  # 원래 지역 이름 사용\n",
    "similarity = probs[0, max_index].item() * 100  # 유사도 값을 퍼센트로 변환\n",
    "\n",
    "# 결과 출력\n",
    "if similarity < 70:\n",
    "    print(f\"입력하신 지역들은 이미지와 유사도가 낮아 정확하게 찾아줄 수는 없습니다.\\n 그러나 이 이미지는 현재 입력하신 지역 중 {most_similar_location}가 가장 유사도가 높습니다. 유사도는 {similarity:.2f}%입니다.\")\n",
    "else:\n",
    "    print(f\"이 이미지는 {most_similar_location}가 가장 유사도가 높습니다. 유사도는 {similarity:.2f}%입니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
